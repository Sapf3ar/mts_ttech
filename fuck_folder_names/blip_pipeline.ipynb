{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiexkRNeQSW4",
        "outputId": "6548402e-fd5b-41f5-cad7-ce8aa272aa27"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!cp /content/drive/MyDrive/ttech/videos.tar.gz /content/\n",
        "!tar -xf  /content/videos.tar.gz "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXZw0MknMHdC",
        "outputId": "c4dc2f18-babc-4f14-8c37-56c871a8d074"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMV_y-3l2Qq6",
        "outputId": "8945c184-d738-4ffb-b918-565be0d243df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Running in Colab.\n",
            "fatal: destination path 'BLIP' already exists and is not an empty directory.\n",
            "/content/BLIP\n"
          ]
        }
      ],
      "source": [
        "# install requirements\n",
        "%cd /content/\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !cp /content/drive/MyDrive/ttech/inf_video.mp4 .\n",
        "    !pip3 install transformers==4.15.0 timm==0.4.12 fairscale==0.4.4 -q \n",
        "    # !pip install salesforce-lavis\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ],
      "metadata": {
        "id": "B0V7gvLz2UZB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title decoder + generator code + extractor\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from models.vit import VisionTransformer, interpolate_pos_embed\n",
        "from models.med import BertConfig, BertModel, BertLMHeadModel\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "from timm.models.hub import download_cached_file\n",
        "\n",
        "\n",
        "def init_tokenizer():\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       \n",
        "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  \n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n",
        "        \n",
        "    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n",
        "    if vit=='base':\n",
        "        vision_width = 768\n",
        "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n",
        "                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
        "                                           drop_path_rate=0 or drop_path_rate\n",
        "                                          )   \n",
        "    elif vit=='large':\n",
        "        vision_width = 1024\n",
        "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n",
        "                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
        "                                           drop_path_rate=0.1 or drop_path_rate\n",
        "                                          )   \n",
        "    return visual_encoder, vision_width\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_url(url_or_filename):\n",
        "    parsed = urlparse(url_or_filename)\n",
        "    return parsed.scheme in (\"http\", \"https\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(model,url_or_filename):\n",
        "    if is_url(url_or_filename):\n",
        "        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n",
        "        checkpoint = torch.load(cached_file, map_location='cpu') \n",
        "    elif os.path.isfile(url_or_filename):        \n",
        "        checkpoint = torch.load(url_or_filename, map_location='cpu') \n",
        "    else:\n",
        "        raise RuntimeError('checkpoint url or path is invalid')\n",
        "        \n",
        "    state_dict = checkpoint['model']\n",
        "    \n",
        "    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n",
        "    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n",
        "        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n",
        "                                                                         model.visual_encoder_m)    \n",
        "    for key in model.state_dict().keys():\n",
        "        if key in state_dict.keys():\n",
        "            if state_dict[key].shape!=model.state_dict()[key].shape:\n",
        "                del state_dict[key]\n",
        "    \n",
        "    msg = model.load_state_dict(state_dict,strict=False)\n",
        "    print('load checkpoint from %s'%url_or_filename)  \n",
        "    return model,msg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BLIP_Base(nn.Module):\n",
        "    def __init__(self,                 \n",
        "                 med_config = 'configs/med_config.json',  \n",
        "                 image_size = 224,\n",
        "                 vit = 'base',\n",
        "                 vit_grad_ckpt = False,\n",
        "                 vit_ckpt_layer = 0,                 \n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
        "            image_size (int): input image size\n",
        "            vit (str): model size of vision transformer\n",
        "        \"\"\"               \n",
        "        super().__init__()\n",
        "        \n",
        "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
        "        self.tokenizer = init_tokenizer()   \n",
        "        med_config = BertConfig.from_json_file(med_config)\n",
        "        med_config.encoder_width = vision_width\n",
        "        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n",
        "\n",
        "        \n",
        "    def forward(self, image, caption, mode):\n",
        "        \n",
        "        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n",
        "        text = self.tokenizer(caption, return_tensors=\"pt\").to(device) \n",
        "        \n",
        "        if mode=='image':    \n",
        "            # return image features\n",
        "            image_embeds = self.visual_encoder(image)             \n",
        "            return image_embeds\n",
        "        \n",
        "        elif mode=='text':\n",
        "            # return text features\n",
        "            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n",
        "                                            return_dict = True, mode = 'text')  \n",
        "            return text_output.last_hidden_state\n",
        "        \n",
        "        elif mode=='multimodal':\n",
        "            # return multimodel features\n",
        "            image_embeds = self.visual_encoder(image)    \n",
        "            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(device)      \n",
        "            \n",
        "            text.input_ids[:,0] = self.tokenizer.enc_token_id\n",
        "            output = self.text_encoder(text.input_ids,\n",
        "                                       attention_mask = text.attention_mask,\n",
        "                                       encoder_hidden_states = image_embeds,\n",
        "                                       encoder_attention_mask = image_atts,      \n",
        "                                       return_dict = True,\n",
        "                                      )              \n",
        "            return output.last_hidden_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def blip_feature_extractor(pretrained='',**kwargs):\n",
        "    model = BLIP_Base(**kwargs)\n",
        "    if pretrained:\n",
        "        model,msg = load_checkpoint(model,pretrained)\n",
        "        assert(len(msg.missing_keys)==0)\n",
        "    return model \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BLIP_Decoder(nn.Module):\n",
        "    def __init__(self,                 \n",
        "                 med_config = 'configs/med_config.json',  \n",
        "                 image_size = 384,\n",
        "                 vit = 'base',\n",
        "                 vit_grad_ckpt = False,\n",
        "                 vit_ckpt_layer = 0,\n",
        "                 prompt = 'a picture of ',\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
        "            image_size (int): input image size\n",
        "            vit (str): model size of vision transformer\n",
        "        \"\"\"            \n",
        "        super().__init__()\n",
        "        \n",
        "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
        "        self.tokenizer = init_tokenizer()   \n",
        "        med_config = BertConfig.from_json_file(med_config)\n",
        "        med_config.encoder_width = vision_width\n",
        "        self.text_decoder = BertLMHeadModel(config=med_config)    \n",
        "        \n",
        "        self.prompt = prompt\n",
        "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1\n",
        "\n",
        "        \n",
        "    def forward(self, image_embeds, caption):\n",
        "        \n",
        "        #image_embeds = self.visual_encoder(image) \n",
        "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(device)\n",
        "        \n",
        "        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(device) \n",
        "        \n",
        "        text.input_ids[:,0] = self.tokenizer.bos_token_id\n",
        "        \n",
        "        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n",
        "        decoder_targets[:,:self.prompt_length] = -100\n",
        "     \n",
        "        decoder_output = self.text_decoder(text.input_ids, \n",
        "                                           attention_mask = text.attention_mask, \n",
        "                                           encoder_hidden_states = image_embeds,\n",
        "                                           encoder_attention_mask = image_atts,                  \n",
        "                                           labels = decoder_targets,\n",
        "                                           return_dict = True,   \n",
        "                                          )   \n",
        "        loss_lm = decoder_output.loss\n",
        "        \n",
        "        return loss_lm\n",
        "        \n",
        "    def generate(self, image_embeds, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
        "        #image_embeds = self.visual_encoder(image)\n",
        "        batch_size = image_embeds.shape[0]\n",
        "\n",
        "        if not sample:\n",
        "            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n",
        "            \n",
        "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(device)\n",
        "        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n",
        "        \n",
        "        prompt = [self.prompt] * batch_size\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
        "        input_ids[:,0] = self.tokenizer.bos_token_id\n",
        "        input_ids = input_ids[:, :-1]\n",
        "\n",
        "        if sample:\n",
        "            #nucleus sampling\n",
        "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
        "                                                  max_length=max_length,\n",
        "                                                  min_length=min_length,\n",
        "                                                  do_sample=True,\n",
        "                                                  top_p=top_p,\n",
        "                                                  num_return_sequences=1,\n",
        "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
        "                                                  pad_token_id=self.tokenizer.pad_token_id, \n",
        "                                                  repetition_penalty=1.1,                                            \n",
        "                                                  **model_kwargs)\n",
        "        else:\n",
        "            #beam search\n",
        "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
        "                                                  max_length=max_length,\n",
        "                                                  min_length=min_length,\n",
        "                                                  num_beams=num_beams,\n",
        "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
        "                                                  pad_token_id=self.tokenizer.pad_token_id,     \n",
        "                                                  repetition_penalty=repetition_penalty,\n",
        "                                                  **model_kwargs)            \n",
        "            \n",
        "        captions = []    \n",
        "        for output in outputs:\n",
        "            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n",
        "            captions.append(caption[len(self.prompt):])\n",
        "        return captions\n",
        "    \n",
        "\n",
        "def blip_decoder(pretrained='',**kwargs):\n",
        "    model = BLIP_Decoder(**kwargs)\n",
        "    if pretrained:\n",
        "        model,msg = load_checkpoint(model,pretrained)\n",
        "        assert(len(msg.missing_keys)==0)\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gI85GqRL2WhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title video QA code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from models.med import BertConfig, BertModel, BertLMHeadModel\n",
        "from models.blip import create_vit, init_tokenizer, load_checkpoint\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "class BLIP_VQA(nn.Module):\n",
        "    def __init__(self,                 \n",
        "                 med_config = 'configs/med_config.json',  \n",
        "                 image_size = 480,\n",
        "                 vit = 'base',\n",
        "                 vit_grad_ckpt = False,\n",
        "                 vit_ckpt_layer = 0,                   \n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
        "            image_size (int): input image size\n",
        "            vit (str): model size of vision transformer\n",
        "        \"\"\"               \n",
        "        super().__init__()\n",
        "        \n",
        "        self.visual_encoder, vision_width = create_vit(vit, image_size, vit_grad_ckpt, vit_ckpt_layer, drop_path_rate=0.1)\n",
        "        self.tokenizer = init_tokenizer()  \n",
        "        \n",
        "        encoder_config = BertConfig.from_json_file(med_config)\n",
        "        encoder_config.encoder_width = vision_width\n",
        "        self.text_encoder = BertModel(config=encoder_config, add_pooling_layer=False) \n",
        "        \n",
        "        decoder_config = BertConfig.from_json_file(med_config)        \n",
        "        self.text_decoder = BertLMHeadModel(config=decoder_config)          \n",
        "\n",
        "\n",
        "    def forward(self, image_embeds, question, image_size=480, answer=None, n=None, weights=None, train=True, inference='rank', k_test=128):\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        #image_embeds = self.visual_encoder(image) \n",
        "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(device)\n",
        "        \n",
        "        question = self.tokenizer(question, padding='longest', truncation=True, max_length=35, \n",
        "                                  return_tensors=\"pt\").to(device) \n",
        "        question.input_ids[:,0] = self.tokenizer.enc_token_id\n",
        "        \n",
        "        if train:               \n",
        "            '''\n",
        "            n: number of answers for each question\n",
        "            weights: weight for each answer\n",
        "            '''                     \n",
        "            answer = self.tokenizer(answer, padding='longest', return_tensors=\"pt\").to(device) \n",
        "            answer.input_ids[:,0] = self.tokenizer.bos_token_id\n",
        "            answer_targets = answer.input_ids.masked_fill(answer.input_ids == self.tokenizer.pad_token_id, -100)      \n",
        "\n",
        "            question_output = self.text_encoder(question.input_ids, \n",
        "                                                attention_mask = question.attention_mask, \n",
        "                                                encoder_hidden_states = image_embeds,\n",
        "                                                encoder_attention_mask = image_atts,                             \n",
        "                                                return_dict = True)    \n",
        "\n",
        "            question_states = []                \n",
        "            question_atts = []  \n",
        "            for b, n in enumerate(n):\n",
        "                question_states += [question_output.last_hidden_state[b]]*n\n",
        "                question_atts += [question.attention_mask[b]]*n                \n",
        "            question_states = torch.stack(question_states,0)    \n",
        "            question_atts = torch.stack(question_atts,0)     \n",
        "\n",
        "            answer_output = self.text_decoder(answer.input_ids, \n",
        "                                              attention_mask = answer.attention_mask, \n",
        "                                              encoder_hidden_states = question_states,\n",
        "                                              encoder_attention_mask = question_atts,                  \n",
        "                                              labels = answer_targets,\n",
        "                                              return_dict = True,   \n",
        "                                              reduction = 'none',\n",
        "                                             )      \n",
        "            \n",
        "            loss = weights * answer_output.loss\n",
        "            loss = loss.sum()/batch_size\n",
        "\n",
        "            return loss\n",
        "            \n",
        "\n",
        "        else: \n",
        "            question_output = self.text_encoder(question.input_ids, \n",
        "                                                attention_mask = question.attention_mask, \n",
        "                                                encoder_hidden_states = image_embeds,\n",
        "                                                encoder_attention_mask = image_atts,                                    \n",
        "                                                return_dict = True) \n",
        "            \n",
        "            if inference=='generate':\n",
        "                num_beams = 4\n",
        "                question_states = question_output.last_hidden_state.repeat_interleave(num_beams,dim=0)\n",
        "                question_atts = torch.ones(question_states.size()[:-1],dtype=torch.long).to(question_states.device)\n",
        "                model_kwargs = {\"encoder_hidden_states\": question_states, \"encoder_attention_mask\":question_atts}\n",
        "                \n",
        "                bos_ids = torch.full((batch_size,1),fill_value=self.tokenizer.bos_token_id,device=device)\n",
        "                \n",
        "                outputs = self.text_decoder.generate(input_ids=bos_ids,\n",
        "                                                     max_length=150,\n",
        "                                                     min_length=1,\n",
        "                                                     num_beams=num_beams,\n",
        "                                                     eos_token_id=self.tokenizer.sep_token_id,\n",
        "                                                     pad_token_id=self.tokenizer.pad_token_id, \n",
        "                                                     **model_kwargs)\n",
        "                \n",
        "                answers = []    \n",
        "                for output in outputs:\n",
        "                    answer = self.tokenizer.decode(output, skip_special_tokens=True)    \n",
        "                    answers.append(answer)\n",
        "                return answers\n",
        "            \n",
        "            elif inference=='rank':\n",
        "                max_ids = self.rank_answer(question_output.last_hidden_state, question.attention_mask, \n",
        "                                           answer.input_ids, answer.attention_mask, k_test) \n",
        "                return max_ids\n",
        " \n",
        "                \n",
        "                \n",
        "    def rank_answer(self, question_states, question_atts, answer_ids, answer_atts, k):\n",
        "        \n",
        "        num_ques = question_states.size(0)\n",
        "        start_ids = answer_ids[0,0].repeat(num_ques,1) # bos token\n",
        "        \n",
        "        start_output = self.text_decoder(start_ids, \n",
        "                                         encoder_hidden_states = question_states,\n",
        "                                         encoder_attention_mask = question_atts,                                      \n",
        "                                         return_dict = True,\n",
        "                                         reduction = 'none')              \n",
        "        logits = start_output.logits[:,0,:] # first token's logit\n",
        "        \n",
        "        # topk_probs: top-k probability \n",
        "        # topk_ids: [num_question, k]        \n",
        "        answer_first_token = answer_ids[:,1]\n",
        "        prob_first_token = F.softmax(logits,dim=1).index_select(dim=1, index=answer_first_token) \n",
        "        topk_probs, topk_ids = prob_first_token.topk(k,dim=1) \n",
        "        \n",
        "        # answer input: [num_question*k, answer_len]                 \n",
        "        input_ids = []\n",
        "        input_atts = []\n",
        "        for b, topk_id in enumerate(topk_ids):\n",
        "            input_ids.append(answer_ids.index_select(dim=0, index=topk_id))\n",
        "            input_atts.append(answer_atts.index_select(dim=0, index=topk_id))\n",
        "        input_ids = torch.cat(input_ids,dim=0)  \n",
        "        input_atts = torch.cat(input_atts,dim=0)  \n",
        "\n",
        "        targets_ids = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
        "\n",
        "        # repeat encoder's output for top-k answers\n",
        "        question_states = tile(question_states, 0, k)\n",
        "        question_atts = tile(question_atts, 0, k)\n",
        "        \n",
        "        output = self.text_decoder(input_ids, \n",
        "                                   attention_mask = input_atts, \n",
        "                                   encoder_hidden_states = question_states,\n",
        "                                   encoder_attention_mask = question_atts,     \n",
        "                                   labels = targets_ids,\n",
        "                                   return_dict = True, \n",
        "                                   reduction = 'none')   \n",
        "        \n",
        "        log_probs_sum = -output.loss\n",
        "        log_probs_sum = log_probs_sum.view(num_ques,k)\n",
        "\n",
        "        max_topk_ids = log_probs_sum.argmax(dim=1) \n",
        "        max_ids = topk_ids[max_topk_ids>=0,max_topk_ids]\n",
        "\n",
        "        return max_ids\n",
        "    \n",
        "    \n",
        "def blip_vqa(pretrained='',**kwargs):\n",
        "    model = BLIP_VQA(**kwargs)\n",
        "    if pretrained:\n",
        "        model,msg = load_checkpoint(model,pretrained)\n",
        "#         assert(len(msg.missing_keys)==0)\n",
        "    return model  \n",
        "\n",
        "\n",
        "def tile(x, dim, n_tile):\n",
        "    init_dim = x.size(dim)\n",
        "    repeat_idx = [1] * x.dim()\n",
        "    repeat_idx[dim] = n_tile\n",
        "    x = x.repeat(*(repeat_idx))\n",
        "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)]))\n",
        "    return torch.index_select(x, dim, order_index.to(x.device))    \n",
        "        "
      ],
      "metadata": {
        "id": "e3aHYCR82aYZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_video(path, transform=None, frames_num=1):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    \n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    \n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # print(f\"{length=} {fps=}\")\n",
        "    N = length//(frames_num)\n",
        "    # N=5\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "    \n",
        "    current_frame = 1\n",
        "    for i in range(length):\n",
        "        ret, frame = cap.read(current_frame)\n",
        "        \n",
        "        if ret and i==current_frame and len(frames)<frames_num:\n",
        "           \n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = cv2.resize(frame, Config['IMG_SIZE'], interpolation = cv2.INTER_CUBIC)\n",
        "            frame = transform(frame).unsqueeze(0).to(device)\n",
        "            frames.append(frame)\n",
        "            current_frame += N\n",
        "        \n",
        "    cap.release()\n",
        "    return frames"
      ],
      "metadata": {
        "id": "TKUUXqZQ2fTW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frames_texts(frames, model_decoder, model_vqa):\n",
        "    with torch.no_grad():\n",
        "        texts = []\n",
        "        for frame in frames:\n",
        "            caption = model_decoder.generate(frame, sample=False, num_beams=3, max_length=50, min_length=20) \n",
        "            texts.append(caption)\n",
        "        return texts"
      ],
      "metadata": {
        "id": "FNaiFEuq2g74"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fJPPHDhI2kz6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Config = dict(\n",
        "    MAX_FRAMES = 512,\n",
        "    IMG_SIZE = (384, 384),\n",
        ")\n",
        "\n",
        "caption = ''"
      ],
      "metadata": {
        "id": "9Ck-rCej2iPg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def absoluteFilePaths(directory):\n",
        "    for dirpath,_,filenames in os.walk(directory):\n",
        "        for f in filenames:\n",
        "            yield os.path.abspath(os.path.join(dirpath, f))"
      ],
      "metadata": {
        "id": "7wMJJvDeN6mq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_frames(dir_path:str):\n",
        "  frames_coll = dict()\n",
        "  for path in absoluteFilePaths(dir_path):\n",
        "    video_name = path.split('/')[-1].split('.')[0]\n",
        "    frames = read_video(f'{path}', frames_num=Config['MAX_FRAMES'])\n",
        "    frames_coll[video_name] = frames\n",
        "  return frames_coll"
      ],
      "metadata": {
        "id": "f6nESVqvM54g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = collect_frames('/content/video_scenes')"
      ],
      "metadata": {
        "id": "9PKy94i22joI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_scene = frames['inf_video-Scene-005']"
      ],
      "metadata": {
        "id": "MUURIOyzXUbd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
        "    \n",
        "model_decoder = blip_decoder(pretrained=model_url, image_size=Config['IMG_SIZE'][0], vit='base')\n",
        "model_decoder.eval()\n",
        "model_decoder = model_decoder.to(device)"
      ],
      "metadata": {
        "id": "w1Wi3uEG4MPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2dc7e22-6d0a-4a15-f30c-aba97d5f45cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 196 to 576\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'\n",
        "    \n",
        "feature_extractor = blip_feature_extractor(pretrained=model_url, image_size=Config['IMG_SIZE'][0], vit='base')\n",
        "feature_extractor.eval()\n",
        "feature_extractor = feature_extractor.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awFVlsmy4PsZ",
        "outputId": "ac0c33cf-925a-40ff-cb2b-d1d9270f1d13"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 196 to 576\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "    \n",
        "model_vqa = blip_vqa(pretrained=model_url, image_size=Config['IMG_SIZE'][0], vit='base')\n",
        "model_vqa.eval()\n",
        "model_vqa = model_vqa.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW00_J4t4RHC",
        "outputId": "406f01d2-063d-454c-f752-d4d80c8491e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reshape position embedding from 900 to 576\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=utilization.memory --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1snMVpST2wO",
        "outputId": "36729fe0-033b-46a6-c3c6-404e28e9f80a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "utilization.memory [%]\n",
            "1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_description(frames, model_decoder, feature_extractor):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        flag = True\n",
        "        for frame in frames:\n",
        "            if flag:\n",
        "              \n",
        "                frames_embs = feature_extractor(frame, caption, mode='image')\n",
        "                flag = False\n",
        "            else:\n",
        "                frames_embs = torch.cat((frames_embs, feature_extractor(frame, caption, mode='image')), dim=1)\n",
        "        print(frames_embs.size())\n",
        "        with torch.no_grad():\n",
        "            text = model_decoder.generate(frames_embs, num_beams=7, max_length=300, min_length=15, top_p=0.9)\n",
        "\n",
        "        return text"
      ],
      "metadata": {
        "id": "GN7yRQ7S4tr5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def videoQA(frames, model_vqa, feature_extractor, question):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        flag = True\n",
        "        for frame in frames:\n",
        "            if flag:\n",
        "                frames_embs = feature_extractor(frame, caption, mode='image')\n",
        "                flag = False\n",
        "            else:\n",
        "                frames_embs = torch.cat((frames_embs, feature_extractor(frame, caption, mode='image')), dim=1)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            answer = model_vqa(frames_embs, question, train=False, inference='generate')\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "5H9EgoMF4xbZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_scene = frames['inf_video-Scene-008']"
      ],
      "metadata": {
        "id": "aLhpfYc_ZObo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1  = 'How much people are on the photo? Answer with one number'\n",
        "question2  = 'Is there any humans on the picture? Where are they located?'\n",
        "question3 = \"What actions are perfomed on a video?\"\n",
        "\n",
        "question4 = \"Where are the main objects on a video are located?\"\n",
        "question5 = \"What humans are doing on a video?\"\n",
        "question6 = \"What is shown on the picture?\"\n",
        "question3  = 'What is the main event on a video?'\n",
        "\n",
        "questions = [\n",
        "    'What is the main event on a video?',\n",
        "    \"What is shown on the picture?\",\n",
        "    \"What humans are doing on a video?\",\n",
        "    \"Where are the main objects on a video are located?\",\n",
        "    \"What actions are perfomed on a video?\", \n",
        "    'Is there any humans on the picture? Where are they located?',\n",
        "    \"What are the main objects on a video?\"\n",
        "    \"How does scene changes throughout the video?\"\n",
        "    'How much humans are on the photo?',\n",
        "    \"How much non-human objects are on the photo?\"\n",
        "    \"What are the main non-human objects are on the photo?\",\n",
        "    'How much people are on the photo? Answer with one number'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "eK1RkJbW402d"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0rNLcTmyjh-V"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1 = videoQA(test_scene, model_vqa, feature_extractor, question1)"
      ],
      "metadata": {
        "id": "4oESw4RCZQRr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bypUSFA_ZT4_",
        "outputId": "2b233d25-a9b9-48ff-d0b7-85d30dcb5e6a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = video_description(test_scene, model_decoder, feature_extractor)"
      ],
      "metadata": {
        "id": "b1_3vxYY4yDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cec0fc5-e521-4e2a-c2eb-1da7e69d1fac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 577, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "Q_8YFNpWH6r_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bf7410-0200-4866-b259-f1f48dd7a27f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an airport with cars parked on the side of the road']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dummy_input = (torch.randn(1,3, 384, 384, device=\"cuda\"), \" \", 'image')\n",
        "\n",
        "\n",
        "# Providing input and output names sets the display names for values\n",
        "# within the model's graph. Setting these does not change the semantics\n",
        "# of the graph; it is only for readability.\n",
        "#\n",
        "# The inputs to the network consist of the flat list of inputs (i.e.\n",
        "# the values you would pass to the forward() method) followed by the\n",
        "# flat list of parameters. You can partially specify names, i.e. provide\n",
        "# a list here shorter than the number of inputs to the model, and we will\n",
        "# only set that subset of names, starting from the beginning.\n",
        "input_names = [ 'decoder_inputs' ]\n",
        "output_names = [ \"output1\" ]\n",
        "\n",
        "torch.onnx.export(feature_extractor, dummy_input, \"feature_extractor.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
      ],
      "metadata": {
        "id": "sZD2mkleJhi5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/ttech"
      ],
      "metadata": {
        "id": "hBC91o9fO79J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openvino-dev[torch,onnx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F4_0r8wGPA2Q",
        "outputId": "c280bf0d-d3c1-4a8a-a150-b148c6af22d4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openvino-dev[onnx,torch]\n",
            "  Downloading openvino_dev-2022.3.0-9052-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: openvino-dev 2022.3.0 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (8.4.0)\n",
            "Requirement already satisfied: numpy<=1.23.4,>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (1.22.4)\n",
            "Collecting networkx<=2.8.8\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.3\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (4.65.0)\n",
            "Collecting openvino-telemetry>=2022.1.0\n",
            "  Downloading openvino_telemetry-2022.3.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.5 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (4.7.0.72)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (1.10.1)\n",
            "Collecting addict>=2.4.0\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: openvino==2022.3.0 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (2022.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (0.7.1)\n",
            "Collecting jstyleson>=0.0.2\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas~=1.3.5\n",
            "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (2.27.1)\n",
            "Collecting fastjsonschema~=2.15.1\n",
            "  Downloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
            "Collecting onnx<=1.12,>=1.8.1\n",
            "  Downloading onnx-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0.0,>=3.18.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[onnx,torch]) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx<=1.12,>=1.8.1->openvino-dev[onnx,torch]) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->openvino-dev[onnx,torch]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->openvino-dev[onnx,torch]) (2022.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[onnx,torch]) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[onnx,torch]) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[onnx,torch]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[onnx,torch]) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->openvino-dev[onnx,torch]) (1.16.0)\n",
            "Building wheels for collected packages: jstyleson\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2398 sha256=1831341fbd1d78f08704bb1a9554bc6519bd3497fb9b99e5155d25db7b737516\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/2a/06/11202ea86be0f51f34e9411d691e25b991d188d93ab4d3e551\n",
            "Successfully built jstyleson\n",
            "Installing collected packages: texttable, jstyleson, fastjsonschema, addict, onnx, networkx, pandas, openvino-telemetry, openvino-dev\n",
            "  Attempting uninstall: fastjsonschema\n",
            "    Found existing installation: fastjsonschema 2.16.3\n",
            "    Uninstalling fastjsonschema-2.16.3:\n",
            "      Successfully uninstalled fastjsonschema-2.16.3\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "Successfully installed addict-2.4.0 fastjsonschema-2.15.3 jstyleson-0.0.2 networkx-2.8.8 onnx-1.12.0 openvino-dev-2022.3.0 openvino-telemetry-2022.3.0 pandas-1.3.5 texttable-1.6.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openvino",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import openvino\n",
        "from openvino.tools import mo\n",
        "from openvino.runtime import Core, serialize\n",
        "\n",
        "VISION_MODEL_OV = Path(\"blip_vision_model.xml\")\n",
        "ov_vision_model = mo.convert_model('/content/BLIP/feature_extractor.onnx', compress_to_fp16=True)\n",
        "    # save model on disk for next usages\n",
        "serialize(ov_vision_model, str(VISION_MODEL_OV))\n"
      ],
      "metadata": {
        "id": "G3on5D83O_JW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "core = Core()\n",
        "\n",
        "# load models on device\n",
        "ov_vision_model = core.compile_model(VISION_MODEL_OV)"
      ],
      "metadata": {
        "id": "QvRLZfHSQNnU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/BLIP/feature_extractor.onnx /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfUaJ1HKNvUJ",
        "outputId": "e016131c-4fb9-407e-b0b3-5be07026d42d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scenedetect[opencv] --upgrade"
      ],
      "metadata": {
        "id": "jo0cqApWcRQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731044b6-36b4-4c25-dbea-04477eb80495"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scenedetect[opencv]\n",
            "  Downloading scenedetect-0.6.1-py3-none-any.whl (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.1 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (1.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (1.22.4)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (8.1.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (4.7.0.72)\n",
            "Installing collected packages: scenedetect\n",
            "Successfully installed scenedetect-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scenedetect.scene_manager import DEFAULT_MIN_WIDTH\n",
        "DEFAULT_MIN_WIDTH = 10**4"
      ],
      "metadata": {
        "id": "efL1C9dLKN9O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scenedetect import detect, ContentDetector,  split_video_ffmpeg\n",
        "from scenedetect import open_video, SceneManager, split_video_ffmpeg\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from scenedetect.video_splitter import split_video_ffmpeg\n",
        "\n",
        "def split_video_into_scenes(video_path, threshold=30.0):\n",
        "    # Open our video, create a scene manager, and add a detector.\n",
        "    video = open_video(video_path)\n",
        "    scene_manager = SceneManager()\n",
        "    scene_manager.add_detector(\n",
        "        ContentDetector(threshold=threshold))\n",
        "    scene_manager.auto_downscale = False\n",
        "    scene_manager.downscale = 1\n",
        "    scene_manager.detect_scenes(video, show_progress=True)\n",
        "    scene_list = scene_manager.get_scene_list()\n",
        "    # split_video_ffmpeg(video_path, scene_list, show_progress=True)\n",
        "    return scene_list\n",
        "\n",
        "# scene_list = detect('/content/inf_video.mp4', ContentDetector(kernel_size=15, threshold=75))"
      ],
      "metadata": {
        "id": "azC3hchkUxgY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/video_scenes_30.tar.gz /content/drive/MyDrive/ttech/"
      ],
      "metadata": {
        "id": "GO1KX7QWUClz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/video_scenes_30\n",
        "%cd /content/video_scenes_30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl9BFkyURs4q",
        "outputId": "cd5fba3a-c30d-4a44-9330-016f166959c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/video_scenes_30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scenes = split_video_into_scenes('/content/drive/MyDrive/ttech/Zombieland.2009.BDRip.1080p.mkv')"
      ],
      "metadata": {
        "id": "_R1jw8A-D92a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629e3728-6bb6-4bfc-8e0e-7c701d77cf0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detected: 0 | Progress:   0%|          | 0/126168 [00:00<?, ?frames/s]INFO:pyscenedetect:Detecting scenes...\n",
            "Detected: 1580 | Progress: 100%|██████████| 126168/126168 [1:17:41<00:00, 27.06frames/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scenes_timecodes = []\n",
        "for scene in scenes:\n",
        "  time1, time2 = scene\n",
        "  scenes_timecodes.append([time1.get_seconds(), time2.get_seconds()])\n",
        "  "
      ],
      "metadata": {
        "id": "EFxj5xvOUlYp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "with open('/content/timecodes_scenes.json', 'x') as file:\n",
        "  json.dump({'timecodes':scenes_timecodes},file,  ensure_ascii=True)"
      ],
      "metadata": {
        "id": "_MtIp7NhVBF-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/timecodes_scenes.json /content/drive/MyDrive/ttech/timecodes_scenes.json"
      ],
      "metadata": {
        "id": "xP4n3DNRiNu8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf video_scenes_30.tar.gz /content/video_scenes_30/"
      ],
      "metadata": {
        "id": "iEeP9PrkS17d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S4Vj_iibhZj",
        "outputId": "94dd5ad9-386b-49b6-acfb-20977f14fb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['talking to each other']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'1 ans: {answer1[0]}\\n2 ans: {answer2[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC_-cW-K42LK",
        "outputId": "0cb2428a-d85a-4192-b788-c59952a656a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 ans: talk on phone\n",
            "2 ans: man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "\n",
        "# sys.path.append(\"../utils\")\n",
        "# from notebook_utils import download_file\n",
        "\n",
        "# # get model and processor\n",
        "# processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "# model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "# # setup test input: download and read image, prepare question\n",
        "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
        "# download_file(img_url, \"demo.jpg\")\n",
        "# raw_image = Image.open(\"demo.jpg\").convert('RGB')\n",
        "# question = \"how many dogs are in the picture?\"\n",
        "# # preprocess input data\n",
        "# inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
        "\n",
        "# start = time.perf_counter()\n",
        "# # perform generation\n",
        "# out = model.generate(**inputs)\n",
        "# end = time.perf_counter() - start\n",
        "\n",
        "# # postprocess result\n",
        "# answer = processor.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "16lCmTeBNVpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkkc2ZUIRL5O",
        "outputId": "63ea03a7-e0d1-40d3-af38-1fe7d6eec69e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-d2v1uxqn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-d2v1uxqn\n",
            "  Resolved https://github.com/huggingface/transformers to commit b29fd6971d9cd6ba2a824628effe243f543b8f61\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install notebook_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXPv0PrkSmX4",
        "outputId": "d7343354-1658-48f0-8929-1ec440763251"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting notebook_utils\n",
            "  Downloading notebook_utils-0.2.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from notebook_utils) (0.40.0)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from notebook_utils) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from notebook_utils) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from notebook_utils) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from notebook_utils) (1.22.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from deprecated->notebook_utils) (1.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (4.39.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->notebook_utils) (23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->notebook_utils) (2022.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->notebook_utils) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->notebook_utils) (1.16.0)\n",
            "Installing collected packages: deprecated, notebook_utils\n",
            "Successfully installed deprecated-1.2.13 notebook_utils-0.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "mMjGgsMERh7c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "\n",
        "sys.path.append(\"../utils\")\n",
        "\n",
        "\n",
        "# get model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "\n",
        "# setup test input: download and read image, prepare question\n",
        "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
        "# download_file(img_url, \"demo.jpeg\")\n",
        "raw_image = Image.open(\"/content/demo.jpeg\").convert('RGB')\n",
        "question = \"how many dogs are in the picture?\"\n",
        "# preprocess input data\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
        "\n",
        "start = time.perf_counter()\n",
        "# perform generation\n",
        "out = model.generate(**inputs)\n",
        "end = time.perf_counter() - start\n",
        "\n",
        "# postprocess result\n",
        "answer = processor.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZZRXHKRJlP",
        "outputId": "fd90a845-f723-446f-a6ed-936e091ab612"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3oxp_nPTWyJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from openvino.tools import mo\n",
        "import numpy as np\n",
        "from openvino.runtime import Core, serialize\n",
        "\n",
        "VISION_MODEL_OV = Path(\"blip_vision_model.xml\")\n",
        "VISION_MODEL_ONNX = VISION_MODEL_OV.with_suffix(\".onnx\")\n",
        "vision_model = model.vision_model\n",
        "vision_model.eval()\n",
        "\n",
        "# check that model works and save it outputs for reusage as text encoder input\n",
        "with torch.no_grad():\n",
        "    vision_outputs = vision_model(inputs[\"pixel_values\"])\n",
        "\n",
        "# if openvino model does not exist, convert it to onnx and then to IR\n",
        "if not VISION_MODEL_OV.exists():\n",
        "\n",
        "    # export pytorch model to ONNX\n",
        "    if not VISION_MODEL_ONNX.exists():\n",
        "        with torch.no_grad():\n",
        "            torch.onnx.export(vision_model, inputs[\"pixel_values\"], VISION_MODEL_ONNX, input_names=[\"pixel_values\"])\n",
        "    # convert ONNX model to IR using Model Optimizer Python API, use compress_to_fp16=True for compressing model weights to FP16 precision\n",
        "    ov_vision_model = mo.convert_model(VISION_MODEL_ONNX, compress_to_fp16=True)\n",
        "    # save model on disk for next usages\n",
        "    serialize(ov_vision_model, str(VISION_MODEL_OV))\n",
        "    print(f\"Vision model successfuly converted and saved to {VISION_MODEL_OV}\")\n",
        "else:\n",
        "    print(f\"Vision model will be loaded from {VISION_MODEL_OV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCaCCnuNQrLU",
        "outputId": "9e901914-0f6d-447e-8877-91c344d87eec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vision model successfuly converted and saved to blip_vision_model.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_ENCODER_OV = Path(\"blip_text_encoder.xml\")\n",
        "TEXT_ENCODER_ONNX = TEXT_ENCODER_OV.with_suffix(\".onnx\")\n",
        "\n",
        "text_encoder = model.text_encoder\n",
        "text_encoder.eval()\n",
        "\n",
        "# if openvino model does not exist, convert it to onnx and then to IR\n",
        "if not TEXT_ENCODER_OV.exists():\n",
        "    if not TEXT_ENCODER_ONNX.exists():\n",
        "        # prepare example inputs for ONNX export\n",
        "        image_embeds = vision_outputs[0]\n",
        "        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
        "        input_dict = {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\": image_attention_mask}\n",
        "        # specify variable length axes\n",
        "        dynamic_axes = {\"input_ids\": {1: \"seq_len\"}, \"attention_mask\": {1: \"seq_len\"}}\n",
        "        # export PyTorch model to ONNX\n",
        "        with torch.no_grad():\n",
        "            torch.onnx.export(text_encoder, input_dict, TEXT_ENCODER_ONNX, input_names=list(input_dict), dynamic_axes=dynamic_axes)\n",
        "    # convert ONNX model to IR using Model Optimizer Python API, use compress_to_fp16=True for compressing model weights to FP16 precision\n",
        "    ov_text_encoder = mo.convert_model(TEXT_ENCODER_ONNX, compress_to_fp16=True)\n",
        "    # save model on disk for next usages\n",
        "    serialize(ov_text_encoder, str(TEXT_ENCODER_OV))\n",
        "    print(f\"Text encoder successfuly converted and saved to {TEXT_ENCODER_OV}\")\n",
        "else:\n",
        "    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2NcDFLBQyJR",
        "outputId": "20a709f3-965a-49ab-a198-aef51713f9ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/blip/modeling_blip_text.py:704: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if is_decoder:\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/blip/modeling_blip_text.py:623: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if is_decoder:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text encoder successfuly converted and saved to blip_text_encoder.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_decoder = model.text_decoder\n",
        "text_decoder.eval()\n",
        "\n",
        "TEXT_DECODER_OV = Path(\"blip_text_decoder.xml\")\n",
        "TEXT_DECODER_ONNX = TEXT_DECODER_OV.with_suffix(\".onnx\")\n",
        "\n",
        "# prepare example inputs for ONNX export\n",
        "input_ids = torch.tensor([[30522]])  # begin of sequence token id\n",
        "attention_mask = torch.tensor([[1]])  # attention mask for input_ids\n",
        "encoder_hidden_states = torch.rand((1, 10, 768))  # encoder last hidden state from text_encoder\n",
        "encoder_attention_mask = torch.ones((1, 10), dtype=torch.long)  # attention mask for encoder hidden states\n",
        "\n",
        "input_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"encoder_hidden_states\": encoder_hidden_states, \"encoder_attention_mask\": encoder_attention_mask}\n",
        "# specify variable length axes\n",
        "dynamic_axes = {\"input_ids\": {1: \"seq_len\"}, \"attention_mask\": {1: \"seq_len\"}, \"encoder_hidden_states\": {1: \"enc_seq_len\"}, \"encoder_attention_mask\": {1: \"enc_seq_len\"}}\n",
        "\n",
        "# specify output names, logits is main output of model\n",
        "output_names = [\"logits\"]\n",
        "\n",
        "# past key values outputs are output for caching model hidden state\n",
        "past_key_values_outs = []\n",
        "text_decoder_outs = text_decoder(**input_dict)\n",
        "for idx, _ in enumerate(text_decoder_outs[\"past_key_values\"]):\n",
        "    past_key_values_outs.extend([f\"out_past_key_value.{idx}.key\", f\"out_past_key_value.{idx}.value\"])\n",
        "\n",
        "# if openvino model does not exist, convert it to onnx and then to IR\n",
        "if not TEXT_DECODER_OV.exists():\n",
        "    # export PyTorch model to ONNX\n",
        "    if not TEXT_DECODER_ONNX.exists():\n",
        "        with torch.no_grad():\n",
        "            torch.onnx.export(text_decoder, input_dict, TEXT_DECODER_ONNX, input_names=list(input_dict), output_names=output_names + past_key_values_outs, dynamic_axes=dynamic_axes)\n",
        "    # convert ONNX model to IR using Model Optimizer Python API, use compress_to_fp16=True for compressing model weights to FP16 precision\n",
        "    ov_text_decoder = mo.convert_model(TEXT_DECODER_ONNX, compress_to_fp16=True)\n",
        "    # save model on disk for next usages\n",
        "    serialize(ov_text_decoder, str(TEXT_DECODER_OV))\n",
        "    print(f\"Text decoder successfuly converted and saved to {TEXT_DECODER_OV}\")\n",
        "else:\n",
        "    print(f\"Text decoder will be loaded from {TEXT_DECODER_OV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghToXfkhQz4W",
        "outputId": "da305bd5-defb-4ca5-ba39-e3edc2ca8806"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/blip/modeling_blip_text.py:632: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if causal_mask.shape[1] < attention_mask.shape[1]:\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/blip/modeling_blip_text.py:883: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if return_logits:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text decoder successfuly converted and saved to blip_text_decoder.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extend input dictionary with hidden states from previous step\n",
        "input_dict_with_past = {**input_dict, \"past_key_values\": text_decoder_outs[\"past_key_values\"]}\n",
        "\n",
        "# provide names for past_key_value inputs in ONNX model\n",
        "past_inputs = [k.replace(\"out_\", \"in_\") for k in past_key_values_outs]\n",
        "\n",
        "# extend input names list and dynamic axes with new inputs\n",
        "input_names_with_past = list(input_dict) + past_inputs\n",
        "dynamic_axes_with_past = {**dynamic_axes}\n",
        "for k in past_inputs:\n",
        "    dynamic_axes_with_past[k] = {2: \"prev_seq_len\"}\n",
        "\n",
        "TEXT_DECODER_WITH_PAST_OV = Path(\"blip_text_decoder_with_past.xml\")\n",
        "TEXT_DECODER_WITH_PAST_ONNX = TEXT_DECODER_WITH_PAST_OV.with_suffix(\".onnx\")\n",
        "\n",
        "# if openvino model does not exist, convert it to onnx and then to IR\n",
        "if not TEXT_DECODER_WITH_PAST_OV.exists():\n",
        "    # export PyTorch model to ONNX\n",
        "    if not TEXT_DECODER_WITH_PAST_ONNX.exists():\n",
        "        with torch.no_grad():\n",
        "            torch.onnx.export(text_decoder, input_dict_with_past, TEXT_DECODER_WITH_PAST_ONNX, input_names=input_names_with_past, output_names=output_names + past_key_values_outs, dynamic_axes=dynamic_axes_with_past)\n",
        "    # convert ONNX model to IR using Model Optimizer Python API, use compress_to_fp16=True for compressing model weights to FP16 precision\n",
        "    ov_text_decoder = mo.convert_model(TEXT_DECODER_WITH_PAST_ONNX, compress_to_fp16=True)\n",
        "    # save model on disk for next usages\n",
        "    serialize(ov_text_decoder, str(TEXT_DECODER_WITH_PAST_OV))\n",
        "    print(f\"Text decoder with past successfuly converted and saved to {TEXT_DECODER_WITH_PAST_OV}\")\n",
        "else:\n",
        "    print(f\"Text decoder with past will be loaded from {TEXT_DECODER_WITH_PAST_OV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bumNo8vsQ2Ig",
        "outputId": "b99231a8-e4bd-4108-c1ae-d57983befb73"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text decoder with past successfuly converted and saved to blip_text_decoder_with_past.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create OpenVINO Core object instance\n",
        "core = Core()\n",
        "\n",
        "# load models on device\n",
        "ov_vision_model = core.compile_model(VISION_MODEL_OV)\n",
        "ov_text_encoder = core.compile_model(TEXT_ENCODER_OV)\n",
        "ov_text_decoder = core.compile_model(TEXT_DECODER_OV)\n",
        "ov_text_decoder_with_past = core.compile_model(TEXT_DECODER_WITH_PAST_OV)"
      ],
      "metadata": {
        "id": "KT49J0_nQ30a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict\n",
        "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
        "\n",
        "\n",
        "def prepare_past_inputs(past_key_values:List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "    \"\"\"\n",
        "    Helper function for rearrange input hidden states inputs to OpenVINO model expected format\n",
        "    Parameters:\n",
        "      past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): list of pairs key, value attention hidden states obtained as model outputs from previous step\n",
        "    Returns:\n",
        "      inputs (Dict[str, torch.Tensor]): dictionary with inputs for model\n",
        "    \"\"\"\n",
        "    inputs = {}\n",
        "    for idx, (key, value) in enumerate(past_key_values):\n",
        "        inputs[f\"in_past_key_value.{idx}.key\"] = key\n",
        "        inputs[f\"in_past_key_value.{idx}.value\"] = value\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def postprocess_text_decoder_outputs(output:Dict):\n",
        "    \"\"\"\n",
        "    Helper function for rearranging model outputs and wrapping to CausalLMOutputWithCrossAttentions\n",
        "    Parameters:\n",
        "      output (Dict): dictionary with model output\n",
        "    Returns\n",
        "      wrapped_outputs (CausalLMOutputWithCrossAttentions): outputs wrapped to CausalLMOutputWithCrossAttentions format\n",
        "    \"\"\"\n",
        "    outs = {k.any_name: v for k, v in output.items()}\n",
        "    logits = torch.from_numpy(outs[\"logits\"])\n",
        "    past_kv = []\n",
        "    for i in range(0, len(past_key_values_outs), 2):\n",
        "        key = past_key_values_outs[i]\n",
        "        value = key.replace(\".key\", \".value\")\n",
        "        past_kv.append((torch.from_numpy(outs[key]), torch.from_numpy(outs[value])))\n",
        "    return CausalLMOutputWithCrossAttentions(\n",
        "        loss=None,\n",
        "        logits=logits,\n",
        "        past_key_values=past_kv,\n",
        "        hidden_states=None,\n",
        "        attentions=None,\n",
        "        cross_attentions=None\n",
        "    )\n",
        "\n",
        "\n",
        "def text_decoder_forward(input_ids:torch.Tensor, attention_mask:torch.Tensor, past_key_values:List[Tuple[torch.Tensor, torch.Tensor]], encoder_hidden_states:torch.Tensor, encoder_attention_mask:torch.Tensor, **kwargs):\n",
        "    \"\"\"\n",
        "    Inference function for text_decoder in one generation step\n",
        "    Parameters:\n",
        "      input_ids (torch.Tensor): input token ids\n",
        "      attention_mask (torch.Tensor): attention mask for input token ids\n",
        "      past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): list of cached decoder hidden states from previous step\n",
        "      encoder_hidden_states (torch.Tensor): encoder (vision or text) hidden states\n",
        "      encoder_attention_mask (torch.Tensor): attnetion mask for encoder hidden states\n",
        "    Returns\n",
        "      model outputs (CausalLMOutputWithCrossAttentions): model prediction wrapped to CausalLMOutputWithCrossAttentions class including predicted logits and hidden states for caching\n",
        "    \"\"\"\n",
        "    input_dict = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"encoder_hidden_states\": encoder_hidden_states,\n",
        "        \"encoder_attention_mask\": encoder_attention_mask\n",
        "    }\n",
        "    if past_key_values is None:\n",
        "        outputs = ov_text_decoder(input_dict)\n",
        "    else:\n",
        "        input_dict.update(prepare_past_inputs(past_key_values))\n",
        "        outputs = ov_text_decoder_with_past(input_dict)\n",
        "    return postprocess_text_decoder_outputs(outputs)\n",
        "\n",
        "\n",
        "text_decoder.forward = text_decoder_forward\n",
        "\n",
        "\n",
        "class OVBlipModel:\n",
        "    \"\"\"\n",
        "    Model class for inference BLIP model with OpenVINO\n",
        "    \"\"\"\n",
        "    def __init__(self, config, decoder_start_token_id:int, vision_model, text_encoder, text_decoder):\n",
        "        \"\"\"\n",
        "        Initialization class parameters\n",
        "        \"\"\"\n",
        "        self.vision_model = vision_model\n",
        "        self.vision_model_out = vision_model.output(0)\n",
        "        self.text_encoder = text_encoder\n",
        "        self.text_encoder_out = text_encoder.output(0)\n",
        "        self.text_decoder = text_decoder\n",
        "        self.config = config\n",
        "        self.decoder_start_token_id = decoder_start_token_id\n",
        "        self.decoder_input_ids = config.text_config.bos_token_id\n",
        "\n",
        "    def generate_answer(self, pixel_values:torch.Tensor, input_ids:torch.Tensor, attention_mask:torch.Tensor, **generate_kwargs):\n",
        "        \"\"\"\n",
        "        Visual Question Answering prediction\n",
        "        Parameters:\n",
        "          pixel_values (torch.Tensor): preprocessed image pixel values\n",
        "          input_ids (torch.Tensor): question token ids after tokenization\n",
        "          attention_mask (torch.Tensor): attention mask for question tokens\n",
        "        Retruns:\n",
        "          generation output (torch.Tensor): tensor which represents sequence of generated answer token ids\n",
        "        \"\"\"\n",
        "        image_embed = self.vision_model(pixel_values.detach().numpy())[self.vision_model_out]\n",
        "        image_attention_mask = np.ones(image_embed.shape[:-1], dtype=int)\n",
        "        if isinstance(input_ids, list):\n",
        "            input_ids = torch.LongTensor(input_ids)\n",
        "        question_embeds = self.text_encoder([input_ids.detach().numpy(), attention_mask.detach().numpy(), image_embed, image_attention_mask])[self.text_encoder_out]\n",
        "        question_attention_mask = np.ones(question_embeds.shape[:-1], dtype=int)\n",
        "\n",
        "        bos_ids = np.full((question_embeds.shape[0], 1), fill_value=self.decoder_start_token_id)\n",
        "\n",
        "        outputs = self.text_decoder.generate(\n",
        "            input_ids=torch.from_numpy(bos_ids),\n",
        "            eos_token_id=self.config.text_config.sep_token_id,\n",
        "            pad_token_id=self.config.text_config.pad_token_id,\n",
        "            encoder_hidden_states=torch.from_numpy(question_embeds),\n",
        "            encoder_attention_mask=torch.from_numpy(question_attention_mask),\n",
        "            **generate_kwargs,\n",
        "        )\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, pixel_values:torch.Tensor, input_ids:torch.Tensor = None, attention_mask:torch.Tensor = None, **generate_kwargs):\n",
        "        \"\"\"\n",
        "        Image Captioning prediction\n",
        "        Parameters:\n",
        "          pixel_values (torch.Tensor): preprocessed image pixel values\n",
        "          input_ids (torch.Tensor, *optional*, None): pregenerated caption token ids after tokenization, if provided caption generation continue provided text\n",
        "          attention_mask (torch.Tensor): attention mask for caption tokens, used only if input_ids provided\n",
        "        Retruns:\n",
        "          generation output (torch.Tensor): tensor which represents sequence of generated caption token ids\n",
        "        \"\"\"\n",
        "        batch_size = pixel_values.shape[0]\n",
        "\n",
        "        image_embeds = self.vision_model(pixel_values.detach().numpy())[self.vision_model_out]\n",
        "\n",
        "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long)\n",
        "\n",
        "        if isinstance(input_ids, list):\n",
        "            input_ids = torch.LongTensor(input_ids)\n",
        "        elif input_ids is None:\n",
        "            input_ids = (\n",
        "                torch.LongTensor([[self.config.text_config.bos_token_id, self.config.text_config.eos_token_id]])\n",
        "                .repeat(batch_size, 1)\n",
        "            )\n",
        "        input_ids[:, 0] = self.config.text_config.bos_token_id\n",
        "        attention_mask = attention_mask[:, :-1] if attention_mask is not None else None\n",
        "\n",
        "        outputs = self.text_decoder.generate(\n",
        "            input_ids=input_ids[:, :-1],\n",
        "            eos_token_id=self.config.text_config.sep_token_id,\n",
        "            pad_token_id=self.config.text_config.pad_token_id,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_hidden_states=torch.from_numpy(image_embeds),\n",
        "            encoder_attention_mask=image_attention_mask,\n",
        "            **generate_kwargs,\n",
        "        )\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "O5LV3CHoQ5qQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/blip_text_encoder.onnx /content/drive/MyDrive/ttech/"
      ],
      "metadata": {
        "id": "AyHnEkwvUlrT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ov_model = OVBlipModel(model.config, model.decoder_start_token_id, ov_vision_model, ov_text_encoder, text_decoder)\n",
        "out = ov_model.generate_answer(**inputs, max_length=20)"
      ],
      "metadata": {
        "id": "ziJYNVp_Q-b-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = ov_model.generate_caption(inputs[\"pixel_values\"], max_length=20)\n",
        "caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "fig = visualize_results(raw_image, caption)"
      ],
      "metadata": {
        "id": "aAB2xn72Q-04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter()\n",
        "out = ov_model.generate_answer(**inputs, max_length=20)\n",
        "end = time.perf_counter() - start\n",
        "answer = processor.decode(out[0], skip_special_tokens=True)\n",
        "fig = visualize_results(raw_image, answer, question)"
      ],
      "metadata": {
        "id": "yV-yjFQbRAk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}